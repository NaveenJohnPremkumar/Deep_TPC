#!/bin/bash

model_name=GPT2WithMM

module load python/3.10.4
# module load numpy
# source /env/bin/activate

# training
python -u run.py \
  --task_name long_term_forecast \
  --is_training 1 \
  --root_path ./dataset/ETT-small/ \
  --data_path ETTh1.csv \
  --model_id ETTh1_672_96 \
  --model $model_name \
  --data ETTh1 \
  --seq_len 672 \
  --label_len 576 \
  --token_len 96 \
  --test_seq_len 672 \
  --test_label_len 576 \
  --test_pred_len 96 \
  --batch_size 256 \
  --learning_rate 0.0005 \
  --mlp_hidden_layers 0 \
  --train_epochs 10 \
  --use_amp \
  --gpu 0 \
  --cosine \
  --tmax 10 \
  --mix_embeds \
  --drop_last \
  --mm_layers 6 7 8 9 10 11 \
  --num_fusion_tokens 20 \
  --llm_ckp_dir gpt2

# testing
for test_pred_len in 96 192 336 720
do
python -u run.py \
  --task_name long_term_forecast \
  --is_training 0 \
  --root_path ./dataset/ETT-small/ \
  --data_path ETTh1.csv \
  --model_id ETTh1_672_96 \
  --model $model_name \
  --data ETTh1 \
  --seq_len 672 \
  --label_len 576 \
  --token_len 96 \
  --test_seq_len 672 \
  --test_label_len 576 \
  --test_pred_len $test_pred_len \
  --batch_size 256 \
  --learning_rate 0.0005 \
  --mlp_hidden_layers 0 \
  --train_epochs 10 \
  --use_amp \
  --gpu 0 \
  --cosine \
  --tmax 10 \
  --mix_embeds \
  --drop_last \
  --mm_layers 6 7 8 9 10 11 \
  --num_fusion_tokens 20 \
  --llm_ckp_dir gpt2 \
  --test_dir long_term_forecast_ETTh1_672_96_GPT2WithMM_ETTh1_sl672_ll576_tl96_lr0.0005_bt256_wd0_hd128_hl0_cosTrue_mixTrue_test_0
done
